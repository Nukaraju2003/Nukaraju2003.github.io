{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nukaraju2003/Nukaraju2003.github.io/blob/main/NGram_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "import string\n",
        "from functools import reduce\n",
        "from math import log\n",
        "import itertools"
      ],
      "metadata": {
        "id": "gqGT6zloLrkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/train_corpus.txt\""
      ],
      "metadata": {
        "id": "9ZOd6RLKMvDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_file(filename):\n",
        "    with open(filename) as f:\n",
        "        lines = [line.rstrip() for line in f]\n",
        "    print(\"No of sentences in Corpus: \"+str(len(lines)))\n",
        "    return lines\n",
        "lines = load_file(filename)"
      ],
      "metadata": {
        "id": "eiyZKQetMxqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizes the sentences meaning split the sentences into words seperated by the \"white sapce\".\n",
        "def tokenize_sentence(lines):\n",
        "    lines = [i.strip(\"''\").split(\" \") for i in lines] \n",
        "    print(\"No of sentences in Corpus: \"+str(len(lines)))\n",
        "    return lines\n",
        "lines = tokenize_sentence(lines)"
      ],
      "metadata": {
        "id": "BHEWJ5kZMzDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(string.punctuation)\n",
        "def prep_data(lines):\n",
        "    for i in range(len(lines)):\n",
        "        lines[i] = [''.join(c for c in s if c not in string.punctuation) for s in lines[i]] # remove punctuations\n",
        "        lines[i] = [s for s in lines[i] if s] # removes empty strings\n",
        "        lines[i] = [word.lower() for word in lines[i]] # lower case\n",
        "        lines[i] += ['</s>'] # Append </s> at the end of each sentence in the corpus\n",
        "        lines[i].insert(0, '<s>')  # Append <s> at the beginning of each sentence in the corpus\n",
        "    print(\"No of sentences in Corpus: \"+str(len(lines)))\n",
        "    return lines\n",
        "lines = prep_data(lines)\n",
        "print(lines[0])"
      ],
      "metadata": {
        "id": "kHZysIRgM0iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_file(filename)\n",
        "dataset = tokenize_sentence(dataset)\n",
        "dataset = prep_data(dataset)"
      ],
      "metadata": {
        "id": "imllk6d3M1uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vocabulary(dataset):\n",
        "    dataset_vocab = set(itertools.chain.from_iterable(dataset))\n",
        "    print('vocab....', len(set(dataset_vocab)),dataset_vocab)\n",
        "    return dataset_vocab\n",
        "\n",
        "dataset_vocab = vocabulary(dataset)\n",
        "print('dataset_viacv',len(set(dataset_vocab)), dataset_vocab)"
      ],
      "metadata": {
        "id": "7VRhF5pMM3Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset_vocab)"
      ],
      "metadata": {
        "id": "e28mpbmmM4XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def freq_of_unique_words(lines):\n",
        "    bag_of_words = list(itertools.chain.from_iterable(lines)) # change the nested list to one single list\n",
        "    corpus_word_count = 0 # No of words in the corpus excluding <s> and </s>.\n",
        "\n",
        "    count = {}\n",
        "    for word in bag_of_words:\n",
        "        if word in count :\n",
        "          count[word] += 1\n",
        "        else:\n",
        "          count[word] = 1\n",
        "        if word != '<s>' and word != '</s>':\n",
        "          corpus_word_count +=1\n",
        "            \n",
        "    unique_word_count = len(count) - 2 # number of unique words in the corpus excluding <s> and </s>\n",
        "\n",
        "    print(\"No of unique words in corpus : \"+ str(unique_word_count))\n",
        "    print(\"No of words in corpus: \"+ str(corpus_word_count))\n",
        "    \n",
        "    return count\n",
        "count = freq_of_unique_words(lines)"
      ],
      "metadata": {
        "id": "XdZJS8-fM52b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_word_frequency = freq_of_unique_words(dataset)\n",
        "unique_word_frequency"
      ],
      "metadata": {
        "id": "AwNP0bV5M7Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bigram_frequencies(lines):\n",
        "    bigram_frequencies = dict() \n",
        "    #unique_bigrams = set()\n",
        "    for sentence in lines:re the space and time complexity and record in a tabular format, i.e. how much space and time the program took to generate the language model for unigram, bigram, trigram, 4-gram, and 5-\n",
        "        given_word = None\n",
        "        for word in sentence:\n",
        "            if given_word != None:\n",
        "                bigram_frequencies[(given_word, word)] = bigram_frequencies.get((given_word, word),0) + 1\n",
        "            given_word = word    \n",
        "    print(len(bigram_frequencies))\n",
        "    return bigram_frequencies\n"
      ],
      "metadata": {
        "id": "V_qIPCcqM8Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_frequencies = compute_bigram_frequencies(dataset)\n",
        "bigram_unique_word_count = len(unique_word_frequency)"
      ],
      "metadata": {
        "id": "-gwrIPlpM9p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bigram_probabilities(bigram_frequencies,count):\n",
        "    bigram_probabilities = dict() re the space and time complexity and record in a tabular format, i.e. how much space and time the program took to generate the language model for unigram, bigram, trigram, 4-gram, and 5-\n",
        "    for key in bigram_frequencies:\n",
        "        numerator = bigram_frequencies.get(key)\n",
        "        denominator = count.get(key[0]) # count.get(key[0]) will get the frequency of \"given word\" in the corpus.\n",
        "        if (numerator ==0 or denominator==0):\n",
        "            bigram_probabilities[key] = 0\n",
        "        else:\n",
        "            bigram_probabilities[key] = float(numerator)/float(denominator)\n",
        "    return bigram_probabilities\n",
        "bigram_probabilities = compute_bigram_probabilities(bigram_frequencies,count)"
      ],
      "metadata": {
        "id": "Kq9wDq4CM_XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bigram_count_test_sentence(given_word,word,smoothing):\n",
        "    if smoothing==0:\n",
        "        return 0 if bigram_frequencies.get((given_word,word))==None else bigram_frequencies.get((given_word,word))\n",
        "    elif smoothing == 1:\n",
        "        return 1 if bigram_frequencies.get((given_word,word))==None else bigram_frequencies.get((given_word,word))+1"
      ],
      "metadata": {
        "id": "ZsGrBH2WNCpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_bigram_freq_test_sentence(test_sentence_vocab,smoothing):\n",
        "    print(\"A table showing the bigram counts for test sentence.\"+\"\\nsmoothing =\"+str(smoothing))\n",
        "    print(\"\\t\\t\\t\", end=\"\")\n",
        "    for word in test_sentence_vocab:\n",
        "        if word != '<s>':\n",
        "            print(word, end=\"\\t\\t\")\n",
        "    print(\"\")\n",
        "    for given_word in test_sentence_vocab:\n",
        "        if given_word != '</s>':\n",
        "            if smoothing==1:\n",
        "                print(unique_word_frequency.get(given_word)+bigram_unique_word_count, end =\"\\t\")\n",
        "            elif smoothing==0:\n",
        "                print(unique_word_frequency.get(given_word), end =\"\\t\")\n",
        "            print(given_word, end=\"\\t\\t\")\n",
        "            for word in test_sentence_vocab:\n",
        "                if word !='<s>':\n",
        "                    print(\"{0:}\".format(compute_bigram_count_test_sentence(given_word,word,smoothing)), end=\"\\t\\t\")\n",
        "            print(\"\")\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "JEpXe9UuNJKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram probabilities of the test sentence computed using the bigram probabilities of the training data.\n",
        "# add-one smoothing if 1, no smoothing if 0 ---- smoothing\n",
        "def compute_bigram_prob_test_sentence(given_word,word,smoothing):\n",
        "    bigram_freq = 0 if bigram_frequencies.get((given_word,word))==None else bigram_frequencies.get((given_word,word))\n",
        "    uni_freq = 0 if unique_word_frequency.get((given_word))==None else unique_word_frequency.get((given_word))\n",
        "    if smoothing==0:\n",
        "        return 0 if bigram_probabilities.get((given_word,word))==None else bigram_probabilities.get((given_word,word))\n",
        "    elif smoothing == 1:\n",
        "        numerator = bigram_freq+1\n",
        "        denominator = uni_freq+bigram_unique_word_count\n",
        "        return 0.0 if numerator == 0 or denominator == 0 else float(numerator) / float(denominator)"
      ],
      "metadata": {
        "id": "c8hT2rpBNKvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A table showing the bigram probabilities for test sentence.\n",
        "def print_bigram_probabilities_test_sentence(test_sentence_vocab,smoothing):\n",
        "    print(\"A table showing the bigram probabilities for test sentence\"+\"\\nsmoothing =\"+str(smoothing))\n",
        "    print(\"\\t\\t\", end=\"\")\n",
        "    for word in test_sentence_vocab:\n",
        "        if word != '<s>':\n",
        "            print(word, end=\"\\t\\t\")\n",
        "    print(\"\")\n",
        "    for given_word in test_sentence_vocab:\n",
        "        if given_word != '</s>':\n",
        "            print(given_word, end=\"\\t\\t\")\n",
        "            for word in test_sentence_vocab:\n",
        "                if word !='<s>':\n",
        "                    print(\"{0:.5f}\".format(compute_bigram_prob_test_sentence(given_word,word,smoothing)), end=\"\\t\\t\")\n",
        "            print(\"\")\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "Pt1vCmOPNMY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re the space and time complexity and record in a tabular format, i.e. how much space and time the program took to generate the language model for unigram, bigram, trigram, 4-gram, and 5-# Print the probability of the test sentence\n",
        "# for add-one smoothing if 1, no smoothing if 0\n",
        "def compute_prob_test_sentence(sentence,smoothing):\n",
        "    test_sent_prob = 0\n",
        "    \n",
        "    if(smoothing == 0):\n",
        "        given_word = None\n",
        "        for word in sentence:\n",
        "            if given_word!=None:\n",
        "                if bigram_probabilities.get((given_word,word))==0 or bigram_probabilities.get((given_word,word))== None:\n",
        "                    return 0\n",
        "                else:\n",
        "                    test_sent_prob+=log((bigram_probabilities.get((given_word,word),0)),10)\n",
        "            given_word = word\n",
        "            \n",
        "    elif(smoothing ==1):\n",
        "        given_word = None\n",
        "        for word in sentence:\n",
        "            if given_word!=None:\n",
        "                bigram_freq = 0 if bigram_frequencies.get((given_word,word))==None else bigram_frequencies.get((given_word,word))\n",
        "                uni_freq = 0 if unique_word_frequency.get((given_word))==None else unique_word_frequency.get((given_word))\n",
        "                numerator = bigram_freq+1\n",
        "                denominator = uni_freq+bigram_unique_word_count\n",
        "                probability = 0 if numerator==0 or denominator ==0 else float(numerator)/float(denominator)\n",
        "                if(probability==0):\n",
        "                    return 0\n",
        "                test_sent_prob +=log(probability,10)\n",
        "            given_word = word\n",
        "            \n",
        "    return 10**test_sent_prob"
      ],
      "metadata": {
        "id": "jJq9YzxjNNu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [['upon this the captain started , and eagerly desired to know more .'],['thus , because no man can follow another into these halls .']]"
      ],
      "metadata": {
        "id": "e-NyLwqWNPa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (len(test_sentences)):\n",
        "    test_sentence = test_sentences[i]\n",
        "    print(\"!!!!!!!!!!The test Sentence is!!!!!!!!!!\")\n",
        "    print(test_sentence)\n",
        "    test_sentence = tokenize_sentence(test_sentence)\n",
        "    test_sentence = prep_data(test_sentence)\n",
        "\n",
        "    # Vocabulary of test sentence\n",
        "    test_sentence_vocab = vocabulary(test_sentence)\n",
        "\n",
        "    test_sentence = list(itertools.chain.from_iterable(test_sentence))\n",
        "    #test_sentence\n",
        "\n",
        "    # A table showing the bigram counts for test sentence.\n",
        "    print_bigram_freq_test_sentence(test_sentence_vocab,smoothing)\n",
        "\n",
        "    # A table showing the bigram probabilities for test sentence.\n",
        "    print_bigram_probabilities_test_sentence(test_sentence_vocab,smoothing)\n",
        "\n",
        "    # The probability of the sentence under the trained model\n",
        "    print(\"The probability of the sentence under the trained model\"+\"\\nsmoothing =\"+str(smoothing))\n",
        "    print(compute_prob_test_sentence(test_sentence,0))"
      ],
      "metadata": {
        "id": "lSQS4vAeNQh0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}